{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to Linear Regression</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Key Concepts</h2>\n",
    "\n",
    "- Loss function\n",
    "- Gradient descent\n",
    "- Linear regression\n",
    "- Statsmodels\n",
    "- In/out-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from pylab import *\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data = pd.read_csv('../datasets/world_happiness_v2.csv')\n",
    "\n",
    "# We also add a constant\n",
    "data['Explained constant'] = 1\n",
    "\n",
    "response =  'Happiness score'\n",
    "predictors = [col for col in data.columns if 'Explained' in col]\n",
    "\n",
    "data.head().columns\n",
    "\n",
    "print(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots of the data\n",
    "figure()\n",
    "data[predictors[0]].hist(bins=10)\n",
    "title(predictors[0])\n",
    "\n",
    "figure()\n",
    "plot(data[predictors[0]], data[response], 'x', alpha=0.5)\n",
    "grid()\n",
    "xlabel(predictors[0])\n",
    "ylabel('response')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Implementation 1:</b> Plot the different predictors. Can you visually tell which predictors are \"important\"?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Fitting a model</h3>\n",
    "\n",
    "We are trying to explain the happiness using the different predictor variables. \n",
    "We start with a linear model, which has the following form:\n",
    "\n",
    "$$ y_i = \\sum_{j=1}^k \\beta_j x_i + \\epsilon_i $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize a random beta vector \n",
    "np.random.seed(0)\n",
    "\n",
    "beta = np.random.rand(len(predictors))\n",
    "\n",
    "# We get an estimate for the happiness conditional on our beta \n",
    "data['y_hat'] = np.dot(data[predictors].values, np.array([beta]).T)\n",
    "\n",
    "# We can plot the estimate against the data\n",
    "figure()\n",
    "plot(data[response], data['y_hat'], 'x', alpha=0.5)\n",
    "grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loss</h3>\n",
    "\n",
    "How good is our model? We can look at its loss. We define the loss in the following way:\n",
    "\n",
    "$$ \\mathrm{L} = \\frac{1}{N}  \\sum_{i=1}^N \\left(y_i - \\sum_{j=1}^k \\beta_j x_i \\right)^2 $$\n",
    "\n",
    "In matrix notation we can write the same as:\n",
    "\n",
    "$$ \\mathrm{L} = \\frac{1}{N} \\left( y - X \\beta \\right)^T \\left(y - X \\beta \\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Implementation 2:</b> Write a method that computes the loss and compute the loss for the random betas\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(X, y, beta):\n",
    "    #TODO\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Gradient descent</h3>\n",
    "\n",
    "We can probably find a beta with a smaller loss. One way is to move along its gradient.\n",
    "\n",
    "The loss can be rewritten as:\n",
    "\n",
    "$$ \\mathrm{L} = \\frac{1}{N} \\left( y^T y -2 \\beta^T X^T y + \\beta^T X^T X \\beta \\right) $$\n",
    "\n",
    "And the grdient as:\n",
    "\n",
    "$$ \\frac{\\partial \\mathrm{L}}{\\partial \\beta} =  \\frac{2}{N} \\left( X^T X \\beta - X^T y  \\right) $$\n",
    "\n",
    "We can then take steps along the gradient with a \"learning rate\" $\\lambda$\n",
    "\n",
    "$$ \\beta_{n+1} = \\beta_n - \\lambda \\left( X^T X \\beta_n - X^T y  \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Implementation 3:</b> Implement the gradient descent algorithm. Plot the loss. Observe what happens if you change the learning rate and n_steps. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3)\n",
    "\n",
    "def get_gradient(X, y, beta):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "def update_beta(X, y, beta, learning_rate):\n",
    "    #TODO\n",
    "    return \n",
    "\n",
    "X = data[predictors].values\n",
    "y = data[response].values\n",
    "beta_update = beta\n",
    "n_steps = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "losses = []\n",
    "for i in range(n_steps):\n",
    "    beta_update = update_beta(X, y, beta_update, learning_rate)\n",
    "    losses.append(get_loss(X, y, beta_update))\n",
    "    \n",
    "plot(losses)\n",
    "grid()\n",
    "xlabel(\"Interation\")\n",
    "ylabel(\"Loss\")\n",
    "\n",
    "print(\"Final loss {}\".format(losses[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Direct solution</h3>\n",
    "\n",
    "We can compute the solution for beta directly by setting the gradient equal to 0 and solve for $\\beta$:\n",
    "\n",
    "$$ \\beta_e =  \\left( X^T X \\right)^{-1} X^T y$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Implementation 4:</b> Implement the direct solution and compare the loss\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_optimal = \n",
    "print(\"loss optimal = \".format(get_loss(X, y, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Statsmodels</h3>\n",
    "\n",
    "There are packages already that run this for you. They also provide you with a lot of usefull information about the model. What do you learn about the relationship between the happiness and its predictors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sm.OLS(data[response], data[predictors]).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the in- and out-sample\n",
    "in_sample = data.ix[0:77]\n",
    "out_sample = data.ix[78:]\n",
    "\n",
    "in_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 1:</b> On the in-sample data, fit one beta per predictor at a time \n",
    "        (univariate regression). Compute \n",
    "        the loss on the out-sample using those betas. Compare the out-sample loss to the\n",
    "        multivariate estimated betas. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 2:</b> On the in-sample data, fit one beta per predictor at a time \n",
    "        (univariate regression) by fitting on the residual of the previous predictor. Compute \n",
    "        the loss on the out-sample using those betas. Compare the out-sample loss to the\n",
    "        multivariate estimated betas. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 5:</b> Change your in-sample model in order to decrease the out-sample loss. Concepts you can try are \n",
    "    <b>manual feature selection</b> and <b>regularization (lasso)</b>. You can also split your in-sample data in half to tune the hyper parameters (alpha). If you are not familiar with those concepts, familiarize yourself first. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = sm.OLS(data[response], data[predictors]).fit_regularized(alpha=0.01)\n",
    "m.params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
