{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to Linear Regression (cont'd)</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Key Concepts</h2>\n",
    "\n",
    "- Goodness of fit\n",
    "- Variable transformations\n",
    "- Multivariate regression\n",
    "- Categorical variables\n",
    "- Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Checking the Goodness of Fit</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the real estate dataset we were looking at last week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/real_estate.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "<b>Exercise 1:</b> Use the statsmodel API to fit a simple linear regression model of price as a function of size.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"Constant\"] = 1\n",
    "m = sm.OLS(..., ...).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model makes two critical assumptions (there are others, but these are two of the most important):\n",
    "\n",
    "- <b>Homoskedasticity</b>: This is a fancy way of saying that the variance of the errors is the same no matter what the value of the predictor variable is.  In other words, the predicted range of house prices is about as wide for a 1,000 square foot house as it is for a 10,000 square foot house.\n",
    "- <b>Independence of errors</b>: This means that knowing how much the model gets one house price wrong shouldn't tell us anything about how much the model gets wrong the prices of other houses.\n",
    "\n",
    "Both of these assumptions are quite strong and rarely hold in practice: the goal is more to make sure they hold reasonably well.  Let's look at the residuals to see how well they hold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "data[\"Residual\"] = m.resid\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(data[\"Size\"], data[\"Residual\"], \"o\", markersize=4)\n",
    "plt.xlabel(\"Size (sq. ft.)\")\n",
    "plt.ylabel(\"Actual minus Predicted Price ($)\")\n",
    "plt.title(\"Size versus Residual Price\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't take college-level statistics to tell that neither of the assumptions above hold that well.  First, the errors are clearly much larger for larger houses than for smaller ones.  Second, it looks like we consistently underpredict prices for houses smaller than 1,000 square feet and consistently overpredict prices for houses between 2,000 and 4,000 square feet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this problem, let's look at histograms of the predictors and responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "data[\"Size\"].plot(kind=\"hist\", rot = -15, bins = 30)\n",
    "plt.title(\"Size (sq. ft.)\")\n",
    "plt.grid()\n",
    "plt.subplot(1, 2, 2)\n",
    "data[\"Price\"].plot(kind=\"hist\", rot = -15, bins = 30)\n",
    "plt.grid()\n",
    "plt.title(\"Price ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these variables have heavy right tails.  This is a not-infrequent occurrence with variables like size and price that can only be positive.  With a variable like price, it's probably more reasonable to expect that errors will be in percentage rather than absolute bands.  While we may be able to predict the error on a \\$500,000 house to within $\\pm$\\$30,000, it would be unrealistic to expect that for King Leopold's house.  However, perhaps we can predict both house prices to within 6%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests transforming both prices and sizes logarithmically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[\"logPrice\"] = np.log(data[\"Price\"])\n",
    "data[\"logSize\"] = np.log(data[\"Size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the histograms of the log transformed data. The distribution looks more normal now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "data[\"logSize\"].plot(kind=\"hist\", rot = -15, bins = 30)\n",
    "plt.title(\"Log Size (sq. ft.)\")\n",
    "plt.grid()\n",
    "plt.subplot(1, 2, 2)\n",
    "data[\"logPrice\"].plot(kind=\"hist\", rot = -15, bins = 30)\n",
    "plt.grid()\n",
    "plt.title(\"Log Price ($)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can fit the model:\n",
    "\n",
    "$$ \\log \\mathrm{Price} = a \\cdot \\log \\mathrm{Size} + b $$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 2:</b> Fit a linear model to the log transformed data. How much does the R-squared change?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 3:</b> Plot the residuals of the log transformed model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually get the house price prediction, we can exponentiate both sides of the fitted model:\n",
    "\n",
    "$$ \\mathrm{price} = e^{4.8892} \\cdot \\mathrm{size}^{1.0491} \\approx = 132.8 \\cdot \\mathrm{size}^{1.0491} $$\n",
    "\n",
    "So prices are pretty close to linear in house size but grow just a bit faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multivariate Regression</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn back to the movie dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv(\"../datasets/tmdb_data.csv\")\n",
    "movie_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see if we can predict the vote average for movies using a multivariate linear model:\n",
    "\n",
    "$$ y_i = \\sum_{j=1}^k a_j x_i + b + \\epsilon_i $$\n",
    "\n",
    "by minimizing the loss:\n",
    "\n",
    "$$ \\mathrm{loss} = \\sum_{i=1}^N \\left(y_i - \\sum_{j=1}^k a_j x_i - b\\right)^2 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the data, by removing NA rows and 0 budget movies. Next, let's shuffle the data and take only the first half of the data to build our model. This is called the \"in-sample\". We'll save the second half of the data, called the \"out-of-sample\", for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_data = movie_data.sample(frac=1, replace = False)\n",
    "movie_data = movie_data.dropna()\n",
    "movie_data = movie_data[movie_data.budget != 0]\n",
    "\n",
    "in_sample = movie_data[0:round(movie_data.shape[0]/2)]\n",
    "out_sample = movie_data[round(movie_data.shape[0]/2):]\n",
    "in_sample['constant'] = 1\n",
    "out_sample['constant'] = 1\n",
    "\n",
    "in_sample[[\"vote_average\",\"duration\", \"budget\", \"title_year\", \"constant\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 4:</b> Using the in-sample data, fit a multivariate model of the vote average as a function of duration, budget, and title year, including an intercept (constant).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems people prefer longer, lower budget, and earlier movies. Let's look at the residuals for this model against the predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.plot(m.predict(in_sample[[\"duration\", \"budget\", \"title_year\", \"constant\"]]), in_sample[\"Residual\"], \"o\", markersize=4)\n",
    "plt.xlabel(\"Predicted Vote Average\")\n",
    "plt.ylabel(\"Residual Vote Average\")\n",
    "plt.title(\"Predicted Vote Average vs Residual Vote Average\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add \"categorical\" variables to our model. The example below illustrates how we can add the country as a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for country in in_sample.country.unique():\n",
    "    print(country)\n",
    "    in_sample[country] = 0.0\n",
    "    out_sample[country] = 0.0\n",
    "\n",
    "    in_sample.loc[in_sample.country == country, country] = 1.0\n",
    "    out_sample.loc[out_sample.country == country, country] = 1.0\n",
    " \n",
    "#An alternative approach\n",
    "#in_sample = in_sample.merge(pd.get_dummies(in_sample.country), how='outer', left_index=True, right_index=True)\n",
    "#out_sample = out_sample.merge(pd.get_dummies(out_sample.country), how='outer', left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our design matrix to have full rank, we need to drop one of the countries. Let's choose the US. This will now serve as our baseline against which comparisons will be made, called our \"reference level\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_sample.drop(labels='United States of America', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to fit our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"duration\", \"budget\", \"title_year\", \"constant\"] + [c for c in in_sample.country.unique() if c != 'United States of America']\n",
    "m = sm.OLS(in_sample[\"vote_average\"], in_sample[predictors]).fit()\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 5:</b> Add a categorical variable representing whether the movie is an action movie or not and refit the model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: use in_sample['genres'].str.contains('Action')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we add new variables, we improve the R-squared of the model, as our model fits our data better. However, by doing so, we risk \"overfitting\" to our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 6:</b> Take a minute to think about why over-fitting is bad.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are many ways to handle overfitting -- many more than can be covered here -- one way is to try to force the slopes in the linear model to be zero unless the predictor really matters.  One way to do this, popularized in the mid-1990s, is to add a term to the sum of squares loss function that penalizes large slopes:\n",
    "\n",
    "$$ \\mathrm{loss} = \\sum_{i=1}^N \\left(y_i - \\sum_{j=1}^k a_j x_i - b\\right)^2 + \\lambda \\sum_{j=1}^k |a_j| $$\n",
    "\n",
    "where $\\lambda$ is called a <b>regularization coefficient</b>.  This is called a <b>lasso regression</b>.  Unfortunately, the fact that we penalize the absolute values (rather than, say, the squares) of the slopes means we can no longer write down closed-form solutions for the optimal slopes and intercept.  However, fast algorithms exist to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know what value for $\\lambda$ we should choose. A popular way is to look at the out-of-sample data and choose the lambda which minimizes the error in the out-of-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 7:</b> Compute the mean squared error of the model on the out-of-sample.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: use out_sample['vote_average_hat'] = m.predict(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 3px solid green; padding: 10px\">\n",
    "  <b>Exercise 8:</b> Loop over some valus for $\\lambda$ and plot the out-of-sample mean squared error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mses = []\n",
    "for regularization in range(30):\n",
    "    m = sm.OLS(..., ...).fit_regularized(alpha = regularization)\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(mses)), mses)\n",
    "plt.xlabel('Regularization')\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the chart above we see that regularization can help reducing the out-of-sample error. In a way, the regularization \"simplifies\" our model, and for predictions, a simpler model can often perform better than a more complex one, which tends to \"overfit\" to the in-sample data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
